{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CS 7641 HW3 Code\n",
    "\n",
    "I'd like to provide proper attribution of code use to Kyle West. His original code can be found at the following link.\n",
    "https://github.com/kylewest520/CS-7641---Machine-Learning/blob/master/Assignment%203%20Unsupervised%20Learning/CS%207641%20HW3%20Code.ipynb\n",
    "\n",
    "\n",
    "\n",
    "This file will provide analysis for 2 Different clustering algorithms and 4 dimensionality reduction algorithms on two different datasets.\n",
    "\n",
    "Datasets: Wine Quality, Credit Card Defaults.\n",
    "\n",
    "Clustering Algorithms: K-Means, Expectation Maximization\n",
    "\n",
    "Dimensionality Reduction: Principal Components Analysis, Independent Components Analysis, Random Components Analysis, Random Forest Classifier Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import itertools\n",
    "import timeit\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---- change to your directory where data files are located ---#\n",
    "#os.chdir(r\"C:\\...\") #change this to your current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read data in and replace last column, drop ID column\n",
    "df_default = pd.read_csv(\"defaultCreditCards.csv\")\n",
    "df_default.columns = ['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0',\n",
    "       'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
    "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
    "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
    "       'y']\n",
    "df_default.drop('ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_default.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to scale numeric features\n",
    "def scaleDefaultData(df, scale=False, encode = False):    \n",
    "    if scale == True:\n",
    "        numCols = ['LIMIT_BAL', 'AGE', 'BILL_AMT1', 'BILL_AMT2',\n",
    "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
    "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "        df_num = df[numCols]\n",
    "        df_stand = (df_num-df_num.min()) / (df_num.max()-df_num.min())\n",
    "        df_cat = df.drop(numCols, axis=1)  \n",
    "        df = pd.concat([df_stand, df_cat], axis=1)\n",
    "        if encode == True:\n",
    "            col_1hot =  ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0',\n",
    "                           'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "            df_1hot = df[col_1hot].astype('category')\n",
    "            df_1hot = pd.get_dummies(df_1hot).astype('int')\n",
    "            df_others = df.drop(col_1hot, axis=1)\n",
    "            df = pd.concat([df_1hot, df_others], axis=1)\n",
    "    return df\n",
    "\n",
    "df_default_scaled = scaleDefaultData(df_default, scale=True, encode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load in wine quality dataset\n",
    "df_wine = pd.read_csv('wineQuality.txt', sep=';')\n",
    "\n",
    "#If wine rating > 5, set quality = 1, else 0.\n",
    "df_wine['quality'] = df_wine['quality'].apply(lambda x: 1 if x > 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to scale wine numeric features\n",
    "def scaleWineData(df):\n",
    "    numCols = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
    "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
    "       'pH', 'sulphates', 'alcohol']\n",
    "    df_num = df[numCols]\n",
    "    df_stand = (df_num-df_num.min()) / (df_num.max()-df_num.min())\n",
    "    df_cat = df.drop(numCols, axis=1)  \n",
    "    df = pd.concat([df_stand, df_cat], axis=1)\n",
    "    return df\n",
    "\n",
    "df_wine_scaled = scaleWineData(df_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split both datasets into x and y.\n",
    "def splitData(df_default, df_wine):\n",
    "    X1 = np.array(df_default.values[:, 0:-1])\n",
    "    y1 = np.array(df_default.values[:, -1])\n",
    "    X2 = np.array(df_wine.values[:, 0:-1])\n",
    "    y2 = np.array(df_wine.values[:, -1])\n",
    "    \n",
    "    return X1, y1, X2, y2\n",
    "\n",
    "#Train test split\n",
    "def trainTestSplit(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=100)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split both scaled datasets into X and Y.\n",
    "X_default, y_default, X_wine, y_wine = splitData(df_wine_scaled, df_wine_scaled)\n",
    "\n",
    "#---Create train and test sets for both datasets---#\n",
    "\n",
    "#Default\n",
    "X_default_train, X_default_test, y_default_train, y_default_test = trainTestSplit(X_default, y_default)\n",
    "\n",
    "#Wine\n",
    "X_wine_train, X_wine_test, y_wine_train, y_wine_test = trainTestSplit(X_wine, y_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--Helper functions --##\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import itertools\n",
    "import timeit\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "def plot_learning_curve(clf, X, y, title=\"Insert Title\"):\n",
    "    \n",
    "    n = len(y)\n",
    "    train_mean = []; train_std = [] #model performance score (f1)\n",
    "    cv_mean = []; cv_std = [] #model performance score (f1)\n",
    "    fit_mean = []; fit_std = [] #model fit/training time\n",
    "    pred_mean = []; pred_std = [] #model test/prediction times\n",
    "    train_sizes=(np.linspace(.05, 1.0, 20)*n).astype('int')  \n",
    "    \n",
    "    for i in train_sizes:\n",
    "        #print(i)\n",
    "        idx = np.random.randint(X.shape[0], size=i)\n",
    "        X_subset = X[idx,:]\n",
    "        y_subset = y[idx]\n",
    "        scores = cross_validate(clf, X_subset, y_subset, cv=10, scoring='f1', n_jobs=-1, return_train_score=True)\n",
    "        \n",
    "        train_mean.append(np.mean(scores['train_score'])); train_std.append(np.std(scores['train_score']))\n",
    "        cv_mean.append(np.mean(scores['test_score'])); cv_std.append(np.std(scores['test_score']))\n",
    "        fit_mean.append(np.mean(scores['fit_time'])); fit_std.append(np.std(scores['fit_time']))\n",
    "        pred_mean.append(np.mean(scores['score_time'])); pred_std.append(np.std(scores['score_time']))\n",
    "    \n",
    "    train_mean = np.array(train_mean); train_std = np.array(train_std)\n",
    "    cv_mean = np.array(cv_mean); cv_std = np.array(cv_std)\n",
    "    fit_mean = np.array(fit_mean); fit_std = np.array(fit_std)\n",
    "    pred_mean = np.array(pred_mean); pred_std = np.array(pred_std)\n",
    "    \n",
    "    plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title)\n",
    "    plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title)\n",
    "    \n",
    "    return train_sizes, train_mean, fit_mean, pred_mean\n",
    "    \n",
    "\n",
    "def plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model F1 Score\")\n",
    "    plt.fill_between(train_sizes, train_mean - 2*train_std, train_mean + 2*train_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, cv_mean - 2*cv_std, cv_mean + 2*cv_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color=\"b\", label=\"Training Score\")\n",
    "    plt.plot(train_sizes, cv_mean, 'o-', color=\"r\", label=\"Cross-Validation Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Modeling Time: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Training Time (s)\")\n",
    "    plt.fill_between(train_sizes, fit_mean - 2*fit_std, fit_mean + 2*fit_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, pred_mean - 2*pred_std, pred_mean + 2*pred_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, fit_mean, 'o-', color=\"b\", label=\"Training Time (s)\")\n",
    "    plt.plot(train_sizes, pred_std, 'o-', color=\"r\", label=\"Prediction Time (s)\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(2), range(2)):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    \n",
    "def final_classifier_evaluation(clf,X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = timeit.default_timer()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    start_time = timeit.default_timer()    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    end_time = timeit.default_timer()\n",
    "    pred_time = end_time - start_time\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "    print(\"Model Evaluation Metrics Using Untouched Test Dataset\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"Model Training Time (s):   \"+\"{:.5f}\".format(training_time))\n",
    "    print(\"Model Prediction Time (s): \"+\"{:.5f}\\n\".format(pred_time))\n",
    "    print(\"F1 Score:  \"+\"{:.2f}\".format(f1))\n",
    "    print(\"Accuracy:  \"+\"{:.2f}\".format(accuracy)+\"     AUC:       \"+\"{:.2f}\".format(auc))\n",
    "    print(\"Precision: \"+\"{:.2f}\".format(precision)+\"     Recall:    \"+\"{:.2f}\".format(recall))\n",
    "    print(\"*****************************************************\")\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"], title='Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "def cluster_predictions(Y,clusterLabels):\n",
    "    assert (Y.shape == clusterLabels.shape)\n",
    "    pred = np.empty_like(Y)\n",
    "    for label in set(clusterLabels):\n",
    "        #print(label)\n",
    "        mask = clusterLabels == label\n",
    "        sub = Y[mask]\n",
    "        target = Counter(sub).most_common(1)[0][0]\n",
    "        pred[mask] = target\n",
    "#    assert max(pred) == max(Y)\n",
    "#    assert min(pred) == min(Y)    \n",
    "    return pred\n",
    "\n",
    "def pairwiseDistCorr(X1,X2):\n",
    "    assert X1.shape[0] == X2.shape[0]\n",
    "    \n",
    "    d1 = pairwise_distances(X1)\n",
    "    d2 = pairwise_distances(X2)\n",
    "    return np.corrcoef(d1.ravel(),d2.ravel())[0,1]\n",
    "\n",
    "def plot_cluster_scatter(X, y, cluster_labels):\n",
    "    plt.scatter(range(len(X)), clusters, c=y)\n",
    "\n",
    "def plot_cluster(km, X_data, y_data, clusters, title=\"First and Second Dimension Clusters\"):\n",
    "    x = []\n",
    "    y = []\n",
    "    for center in km.cluster_centers_:\n",
    "        x.append(center[0])\n",
    "        y.append(center[1])\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(X_data[:, 0], X_data[:,1], c=clusters, cmap='rainbow')\n",
    "    plt.scatter(x, y, marker='*', color='black', s=300, edgecolors='orange')\n",
    "    plt.title(title)\n",
    "\n",
    "def km_complexity(vals, n_clusters=20, n_init=10):\n",
    "    ssd = []\n",
    "    for i in vals:\n",
    "        km = KMeans(n_clusters=n_clusters,n_init=i,random_state=100, n_jobs=-1)\n",
    "        km.fit(X_wine)\n",
    "        ssd.append(km.inertia_)\n",
    "        #preds = km.predict(X_wine)\n",
    "    \n",
    "        #y_mode_vote = cluster_predictions(y_wine, km.labels_)\n",
    "        #acc_score = accuracy_score(y_wine, y_mode_vote)\n",
    "        #print(\"F1 score at \" + str(n_clusters) + \" clusters and \" + str(i) + \" random restarts is \" + str(acc_score))\n",
    "    \n",
    "    plt.plot(vals, ssd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def hyperNN(X_train, y_train, X_test, y_test, title):\n",
    "\n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    hlist = np.linspace(1,150,30).astype('int')\n",
    "    for i in hlist:         \n",
    "            clf = MLPClassifier(hidden_layer_sizes=(i,), solver='adam', activation='logistic', \n",
    "                                learning_rate_init=0.05, random_state=100)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            f1_test.append(f1_score(y_test, y_pred_test))\n",
    "            f1_train.append(f1_score(y_train, y_pred_train))\n",
    "      \n",
    "    plt.plot(hlist, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(hlist, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('No. Hidden Units')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def NNGridSearchCV(X_train, y_train):\n",
    "    #parameters to search:\n",
    "    #number of hidden units\n",
    "    #learning_rate\n",
    "    h_units = [5, 10, 20, 30, 40, 50, 75, 100]\n",
    "    learning_rates = [0.01, 0.05, .1]\n",
    "    param_grid = {'hidden_layer_sizes': h_units, 'learning_rate_init': learning_rates}\n",
    "\n",
    "    net = GridSearchCV(estimator = MLPClassifier(solver='adam',activation='logistic',random_state=100),\n",
    "                       param_grid=param_grid, cv=10, n_jobs=-1)\n",
    "    net.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(net.best_params_)\n",
    "    return net.best_params_['hidden_layer_sizes'], net.best_params_['learning_rate_init']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will implement k-means clustering for both datasets. Our objectives are to:\n",
    "1. Determine the best number of clusters for each dataset by using the elbow inspection method on silhouette score.\n",
    "2. Describe the attributes which make up each cluster.\n",
    "3. Score each cluster with an accuracy since technically we do have labels available for these datasets (labels are not used when determining clusters).\n",
    "\n",
    "Since k-Means is susceptible to get stuck in local optima due to the random selection of initial cluster centers, I will report the average metrics over 5 models for each number of k clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score as sil_score, f1_score, homogeneity_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def run_kmeans(X,y,title):\n",
    "\n",
    "    kclusters = list(np.arange(2,50,2))\n",
    "    sil_scores = []; f1_scores = []; homo_scores = []; train_times = []; sum_squared_distances = []\n",
    "\n",
    "    for k in kclusters:\n",
    "        #print(k)\n",
    "        start_time = timeit.default_timer()\n",
    "        km = KMeans(n_clusters=k, n_init=10,random_state=100,n_jobs=-1).fit(X)\n",
    "        end_time = timeit.default_timer()\n",
    "        train_times.append(end_time - start_time)\n",
    "        #sil_scores.append(sil_score(X, km.labels_))\n",
    "        #y_mode_vote = cluster_predictions(y,km.labels_)\n",
    "        #f1_scores.append(f1_score(y, y_mode_vote))\n",
    "        #homo_scores.append(homogeneity_score(y, km.labels_))\n",
    "        sum_squared_distances.append(km.inertia_)\n",
    "\n",
    "    # plot model training time\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(kclusters, train_times)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Clusters')\n",
    "    plt.ylabel('Training Time (s)')\n",
    "    plt.title('KMeans Training Time: '+ title)\n",
    "    plt.show()\n",
    "    \n",
    "    #Plot squared distances\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(kclusters, sum_squared_distances, 'bx-')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Clusters')\n",
    "    plt.ylabel('Squared Distance')\n",
    "    plt.title('Squared Distances from cluster centers')\n",
    "    plt.show()\n",
    "    \n",
    "    return sum_squared_distances\n",
    "    \n",
    "def evaluate_kmeans(km, X, y):\n",
    "    start_time = timeit.default_timer()\n",
    "    km.fit(X, y)\n",
    "    end_time = timeit.default_timer()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    y_mode_vote = cluster_predictions(y,km.labels_)\n",
    "    #print(y_mode_vote)\n",
    "    auc = roc_auc_score(y, y_mode_vote)\n",
    "    f1 = f1_score(y, y_mode_vote)\n",
    "    accuracy = accuracy_score(y, y_mode_vote)\n",
    "    precision = precision_score(y, y_mode_vote)\n",
    "    recall = recall_score(y, y_mode_vote)\n",
    "    cm = confusion_matrix(y, y_mode_vote)\n",
    "\n",
    "    print(\"Model Evaluation Metrics Using Mode Cluster Vote\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"Model Training Time (s):   \"+\"{:.2f}\".format(training_time))\n",
    "    print(\"No. Iterations to Converge: {}\".format(km.n_iter_))\n",
    "    print(\"F1 Score:  \"+\"{:.2f}\".format(f1))\n",
    "    print(\"Accuracy:  \"+\"{:.2f}\".format(accuracy)+\"     AUC:       \"+\"{:.2f}\".format(auc))\n",
    "    print(\"Precision: \"+\"{:.2f}\".format(precision)+\"     Recall:    \"+\"{:.2f}\".format(recall))\n",
    "    print(\"*****************************************************\")\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"], title='Confusion Matrix')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine_squared_distances = run_kmeans(X_wine, y_wine,'Wine Data')\n",
    "km = KMeans(n_clusters=20,n_init=10,random_state=100,n_jobs=-1)\n",
    "clusters=km.fit_predict(X_wine)\n",
    "evaluate_kmeans(km,X_wine, y_wine)\n",
    "plot_cluster_scatter(X_wine, y_wine, clusters)\n",
    "plot_cluster(km, X_wine, y_wine, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km_complexity([10,20,30,40,50,60,70,80], n_clusters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "default_squared_distances = run_kmeans(X_default, y_default,'Default Data')\n",
    "km = KMeans(n_clusters=25,n_init=10,random_state=100,n_jobs=-1)\n",
    "clusters = km.fit_predict(X_default)\n",
    "evaluate_kmeans(km,X_default,y_default)\n",
    "plot_cluster_scatter(X_default, y_default, clusters)\n",
    "#plot_cluster(km, X_default, y_default, clusters)\n",
    "#km_complexity([10,20,30,40,50,60,70,80])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as EM\n",
    "from sklearn.metrics import silhouette_score as sil_score, f1_score, homogeneity_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def run_EM(X,y,title):\n",
    "\n",
    "    #kdist =  [2,3,4,5]\n",
    "    #kdist = list(range(2,51))\n",
    "    kdist = list(np.arange(2,100,5))\n",
    "    sil_scores = []; f1_scores = []; homo_scores = []; train_times = []; aic_scores = []; bic_scores = []; log_lik = []\n",
    "    \n",
    "    for k in kdist:\n",
    "        start_time = timeit.default_timer()\n",
    "        em = EM(n_components=k,covariance_type='diag',n_init=1,warm_start=True,random_state=100).fit(X)\n",
    "        end_time = timeit.default_timer()\n",
    "        train_times.append(end_time - start_time)\n",
    "        \n",
    "        labels = em.predict(X)\n",
    "        #sil_scores.append(sil_score(X, labels))\n",
    "        #y_mode_vote = cluster_predictions(y,labels)\n",
    "        #f1_scores.append(f1_score(y, y_mode_vote))\n",
    "        #homo_scores.append(homogeneity_score(y, labels))\n",
    "        aic_scores.append(em.aic(X))\n",
    "        bic_scores.append(em.bic(X))\n",
    "        log_lik.append(em.lower_bound_)\n",
    "        \n",
    "    # elbow curve for silhouette score\n",
    "    #fig = plt.figure()\n",
    "    #ax = fig.add_subplot(111)\n",
    "    #ax.plot(kdist, sil_scores)\n",
    "    #plt.grid(True)\n",
    "    #plt.xlabel('No. Distributions')\n",
    "    #plt.ylabel('Avg Silhouette Score')\n",
    "    #plt.title('Elbow Plot for EM: '+ title)\n",
    "    #plt.show()\n",
    "   \n",
    "    # plot homogeneity scores\n",
    "    #fig = plt.figure()\n",
    "    #ax = fig.add_subplot(111)\n",
    "    #ax.plot(kdist, homo_scores)\n",
    "    #plt.grid(True)\n",
    "    #plt.xlabel('No. Distributions')\n",
    "    #plt.ylabel('Homogeneity Score')\n",
    "    #plt.title('Homogeneity Scores EM: '+ title)\n",
    "    #plt.show()\n",
    "\n",
    "    # plot f1 scores\n",
    "    #fig = plt.figure()\n",
    "    #ax = fig.add_subplot(111)\n",
    "    #ax.plot(kdist, f1_scores)\n",
    "    #plt.grid(True)\n",
    "    #plt.xlabel('No. Distributions')\n",
    "    #plt.ylabel('F1 Score')\n",
    "    #plt.title('F1 Scores EM: '+ title)\n",
    "    #plt.show()\n",
    "\n",
    "    # plot model AIC and BIC\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(kdist, aic_scores, label='AIC')\n",
    "    ax.plot(kdist, bic_scores,label='BIC')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Distributions')\n",
    "    plt.ylabel('Model Complexity Score')\n",
    "    plt.title('EM Model Complexity: '+ title)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    # plot model training time\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(kdist, train_times)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Clusters')\n",
    "    plt.ylabel('Training Time (s)')\n",
    "    plt.title('EM Training Time: '+ title)\n",
    "    plt.show()\n",
    "    \n",
    "    #Plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(kdist, log_lik)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('No. Clusters')\n",
    "    plt.ylabel('Log likelihood')\n",
    "    plt.title('EM Log likelihood '+ title)\n",
    "    plt.show()\n",
    "    \n",
    "    return aic_scores, bic_scores, log_lik\n",
    "    \n",
    "def evaluate_EM(em, X, y):\n",
    "    start_time = timeit.default_timer()\n",
    "    em.fit(X, y)\n",
    "    end_time = timeit.default_timer()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    labels = em.predict(X)\n",
    "    y_mode_vote = cluster_predictions(y,labels)\n",
    "    auc = roc_auc_score(y, y_mode_vote)\n",
    "    f1 = f1_score(y, y_mode_vote)\n",
    "    accuracy = accuracy_score(y, y_mode_vote)\n",
    "    precision = precision_score(y, y_mode_vote)\n",
    "    recall = recall_score(y, y_mode_vote)\n",
    "    cm = confusion_matrix(y, y_mode_vote)\n",
    "\n",
    "    print(\"Model Evaluation Metrics Using Mode Cluster Vote\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"Model Training Time (s):   \"+\"{:.2f}\".format(training_time))\n",
    "    print(\"No. Iterations to Converge: {}\".format(em.n_iter_))\n",
    "    print(\"Log-likelihood Lower Bound: {:.2f}\".format(em.lower_bound_))\n",
    "    print(\"F1 Score:  \"+\"{:.2f}\".format(f1))\n",
    "    print(\"Accuracy:  \"+\"{:.2f}\".format(accuracy)+\"     AUC:       \"+\"{:.2f}\".format(auc))\n",
    "    print(\"Precision: \"+\"{:.2f}\".format(precision)+\"     Recall:    \"+\"{:.2f}\".format(recall))\n",
    "    print(\"*****************************************************\")\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"], title='Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine_aic, wine_bic, wine_log_lik = run_EM(X_wine, y_wine,'Wine Data')\n",
    "em = EM(n_components=24,covariance_type='diag',n_init=1,warm_start=True,random_state=100).fit(X_wine)\n",
    "preds = em.predict(X_wine)\n",
    "evaluate_EM(em,X_wine, y_wine)\n",
    "plt.scatter(range(len(X_wine)), preds, c=y_wine, cmap='plasma')\n",
    "#df = pd.DataFrame(em.means_)\n",
    "#print(df)\n",
    "#df.to_csv(\"Phishing EM Component Means.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EM_improvement = {}\n",
    "for covType in ['diag', 'spherical', 'full']:\n",
    "    log_liks = []\n",
    "    for n in range(1, 11):\n",
    "        em = EM(n_components=24,covariance_type=covType,n_init=n,warm_start=True,random_state=100).fit(X_wine)\n",
    "        log_liks.append(em.lower_bound_)\n",
    "        #print(\"EM log-likelihood for \" + covType + \" covariance type and \" + str(n) + \" random restarts is \" + str(em.lower_bound_))\n",
    "    EM_improvement[covType] = log_liks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(1,11)), EM_improvement['diag'], label='diag', c='blue')\n",
    "plt.plot(list(range(1,11)), EM_improvement['spherical'], label='spherical', c='red')\n",
    "plt.plot(list(range(1,11)), EM_improvement['full'], label='full', c='green')\n",
    "plt.ylabel(\"Log likelihood\")\n",
    "plt.xlabel(\"random restarts\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_aic, default_bic, default_log_lik = run_EM(X_default, y_default,'Default Data')\n",
    "em = EM(n_components=20,covariance_type='diag',n_init=1,warm_start=True,random_state=100).fit(X_default)\n",
    "preds = em.predict(X_default)\n",
    "evaluate_EM(em,X_default, y_default)\n",
    "plt.scatter(range(len(X_default)), preds, c=y_default, cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This section will implement 4 different dimensionality reduction techniques on both the phishing and the banking dataset. Then, k-means and EM clustering will be performed for each (dataset * dim_reduction) combination to see how the clustering compares with using the full datasets. The 4 dimensionality reduction techniques are:\n",
    "- Principal Components Analysis (PCA). Optimal number of PC chosen by inspecting % variance explained and the eigenvalues.\n",
    "- Independent Components Analysis (ICA). Optimal number of IC chosen by inspecting kurtosis.\n",
    "- Random Components Analysis (RCA) (otherwise known as Randomized Projections). Optimal number of RC chosen by inspecting reconstruction error.\n",
    "- Random Forest Classifier (RFC). Optimal number of components chosen by feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA as ICA\n",
    "from sklearn.random_projection import GaussianRandomProjection as GRP, SparseRandomProjection as RCA\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "def run_PCA(X,y,title):\n",
    "    \n",
    "    pca = PCA(random_state=5).fit(X) #for all components\n",
    "    cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(list(range(len(pca.explained_variance_ratio_))), cum_var, 'b-')\n",
    "    ax1.set_xlabel('Principal Components')\n",
    "    # Make the y-axis label, ticks and tick labels match the line color.\n",
    "    ax1.set_ylabel('Cumulative Explained Variance Ratio', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    plt.grid(False)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(list(range(len(pca.singular_values_))), pca.singular_values_, 'm-')\n",
    "    ax2.set_ylabel('Eigenvalues', color='m')\n",
    "    ax2.tick_params('y', colors='m')\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.title(\"PCA Explained Variance and Eigenvalues: \"+ title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def run_ICA(X,y,title):\n",
    "    \n",
    "    dims = list(np.arange(2,(X.shape[1]-1),3))\n",
    "    #dims = list(np.arange(2,80,3))\n",
    "    dims.append(X.shape[1])\n",
    "    ica = ICA(random_state=5)\n",
    "    kurt = []\n",
    "\n",
    "    for dim in dims:\n",
    "        print(dim)\n",
    "        ica.set_params(n_components=dim)\n",
    "        tmp = ica.fit_transform(X)\n",
    "        tmp = pd.DataFrame(tmp)\n",
    "        tmp = tmp.kurt(axis=0)\n",
    "        kurt.append(tmp.abs().mean())\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"ICA Kurtosis: \"+ title)\n",
    "    plt.xlabel(\"Independent Components\")\n",
    "    plt.ylabel(\"Avg Kurtosis Across IC\")\n",
    "    plt.plot(dims, kurt, 'b-')\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "def run_RCA(X,y,title):\n",
    "    \n",
    "    dims = list(np.arange(2,(X.shape[1]-1),3))\n",
    "    #dims = list(np.arange(2,80,3))\n",
    "    dims.append(X.shape[1])\n",
    "    tmp = defaultdict(dict)\n",
    "\n",
    "    for i,dim in product(range(5),dims):\n",
    "        #print(i)\n",
    "        rp = RCA(random_state=i, n_components=dim)\n",
    "        tmp[dim][i] = pairwiseDistCorr(rp.fit_transform(X), X)\n",
    "    print(tmp)\n",
    "    tmp = pd.DataFrame(tmp).T\n",
    "    print(tmp)\n",
    "    mean_recon = tmp.mean(axis=1).tolist()\n",
    "    std_recon = tmp.std(axis=1).tolist()\n",
    "\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(dims,mean_recon, 'b-')\n",
    "    ax1.set_xlabel('Random Components')\n",
    "    # Make the y-axis label, ticks and tick labels match the line color.\n",
    "    ax1.set_ylabel('Mean Reconstruction Correlation', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    plt.grid(False)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(dims,std_recon, 'm-')\n",
    "    ax2.set_ylabel('STD Reconstruction Correlation', color='m')\n",
    "    ax2.tick_params('y', colors='m')\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.title(\"Random Components for 5 Restarts: \"+ title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def run_RFC(X,y,df_original):\n",
    "    rfc = RFC(n_estimators=500,min_samples_leaf=round(len(X)*.01),random_state=5,n_jobs=-1)\n",
    "    imp = rfc.fit(X,y).feature_importances_ \n",
    "    imp = pd.DataFrame(imp,columns=['Feature Importance'],index=df_original.columns[:-1])\n",
    "    imp.sort_values(by=['Feature Importance'],inplace=True,ascending=False)\n",
    "    imp['Cum Sum'] = imp['Feature Importance'].cumsum()\n",
    "    imp = imp[imp['Cum Sum']<=0.95]\n",
    "    top_cols = imp.index.tolist()\n",
    "    return imp, top_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_PCA(X_wine, y_wine,\"Wine Data\")\n",
    "run_ICA(X_wine, y_wine,\"Wine Data\")\n",
    "run_RCA(X_wine, y_wine,\"Wine Data\")\n",
    "imp_wine, topcols_wine = run_RFC(X_wine, y_wine, df_wine_scaled)\n",
    "plt.figure(figsize=(5,5))\n",
    "fi = sns.barplot(x=imp_wine.index, y=imp_wine['Feature Importance'])\n",
    "fi.set_xticklabels(fi.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_PCA(X_default, y_default,\"Default Data\")\n",
    "run_ICA(X_default, y_default,\"Default Data\")\n",
    "#run_RCA(X_default, y_default,\"Default Data\")\n",
    "imp_default, topcols_default = run_RFC(X_default, y_default,df_default_scaled)\n",
    "default_fi = sns.barplot(x=imp_default.index[:20], y=imp_default['Feature Importance'][:20])\n",
    "default_fi.set_xticklabels(default_fi.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(random_state=5)\n",
    "pca.fit(X_default)\n",
    "x_pca = pca.transform(X_default)\n",
    "df_comp = pd.DataFrame(pca.components_[:20], columns=df_default_scaled.columns[:-1])\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df_comp,cmap='plasma',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(random_state=5)\n",
    "pca.fit(X_wine)\n",
    "x_pca = pca.transform(X_wine)\n",
    "df_comp = pd.DataFrame(pca.components_, columns=df_wine_scaled.columns[:-1])\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df_comp,cmap='plasma',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_wine, topcols_wine = run_RFC(X_wine, y_wine, df_wine_scaled)\n",
    "pca_wine = PCA(n_components=5,random_state=5).fit_transform(X_wine)\n",
    "ica_wine = ICA(n_components=5,random_state=5).fit_transform(X_wine)\n",
    "ica_wine /= ica_wine.std(axis=0)\n",
    "rca_wine = RCA(n_components=5,random_state=5).fit_transform(X_wine)\n",
    "rfc_wine = df_wine_scaled[topcols_wine]\n",
    "rfc_wine = np.array(rfc_wine.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ss_wine_pca = run_kmeans(pca_wine,y_wine,'PCA Phishing Data')\n",
    "ss_wine_ica = run_kmeans(ica_wine,y_wine,'ICA Phishing Data')\n",
    "ss_wine_rca = run_kmeans(rca_wine,y_wine,'RCA Phishing Data')\n",
    "ss_wine_rfc = run_kmeans(rfc_wine,y_wine,'RFC Phishing Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_distance_comparison(original=None, transformed=None, transformation='pca', aic=None, bic=None, method = 'kmeans'):\n",
    "    \n",
    "    if method == 'kmeans':\n",
    "        kclusters = list(np.arange(2,50,2))\n",
    "\n",
    "        #Plot squared distances\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(kclusters, original, 'bx-', label='original')\n",
    "        ax.plot(kclusters, transformed, label=transformation)\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlabel('No. Clusters')\n",
    "        plt.ylabel('Squared Distance')\n",
    "        plt.title('Squared Distances from cluster centers')\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        kdist = list(np.arange(2,100,5))\n",
    "        #Plot squared distances\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(kdist, aic[0], 'bx-', label='original aic')\n",
    "        ax.plot(kdist, bic[0], 'rx-', label='original bic')\n",
    "        ax.plot(kdist, aic[1], 'bo-',label= transformation + ' aic')\n",
    "        ax.plot(kdist, bic[1], 'ro-',label= transformation + ' bic')\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlabel('No. Distribution')\n",
    "        plt.title('Squared Distances from cluster centers')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squared_distance_comparison(wine_squared_distances, ss_wine_pca, 'pca')\n",
    "squared_distance_comparison(wine_squared_distances, ss_wine_ica, 'ica')\n",
    "squared_distance_comparison(wine_squared_distances, ss_wine_rca, 'rca')\n",
    "squared_distance_comparison(wine_squared_distances, ss_wine_rfc, 'rfc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5)\n",
    "km.fit(pca_wine)\n",
    "preds = km.predict(pca_wine)\n",
    "plot_cluster(km, pca_wine, y_wine, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5)\n",
    "km.fit(ica_wine)\n",
    "preds = km.predict(ica_wine)\n",
    "plot_cluster(km, ica_wine, y_wine, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5)\n",
    "km.fit(rca_wine)\n",
    "preds = km.predict(rca_wine)\n",
    "plot_cluster(km, rca_wine, y_wine, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5)\n",
    "km.fit(rfc_wine)\n",
    "preds = km.predict(rfc_wine)\n",
    "plot_cluster(km, rfc_wine, y_wine, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aic_wine_pca, bic_wine_pca, log_like_wine_pca = run_EM(pca_wine,y_wine,'PCA Phishing Data')\n",
    "aic_wine_ica, bic_wine_ica, log_like_wine_ica  = run_EM(ica_wine,y_wine,'ICA Phishing Data')\n",
    "aic_wine_rca, bic_wine_rca, log_like_wine_rca  = run_EM(rca_wine,y_wine,'RCA Phishing Data')\n",
    "aic_wine_rfc, bic_wine_rfc, log_like_wine_rfc  = run_EM(rfc_wine,y_wine,'RFC Phishing Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squared_distance_comparison(aic=[wine_aic, aic_wine_pca], bic=[wine_bic, bic_wine_pca], transformation='pca', method='em')\n",
    "squared_distance_comparison(aic=[wine_aic, aic_wine_ica], bic=[wine_bic, bic_wine_ica], transformation='ica', method='em')\n",
    "squared_distance_comparison(aic=[wine_aic, aic_wine_rca], bic=[wine_bic, bic_wine_rca], transformation='rca', method='em')\n",
    "squared_distance_comparison(aic=[wine_aic, aic_wine_rfc], bic=[wine_bic, bic_wine_rfc], transformation='rfc', method='em')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = EM(n_components = 8)\n",
    "km.fit(pca_wine)\n",
    "preds = km.predict(pca_wine)\n",
    "plt.scatter(pca_wine[:,0], pca_wine[:,1], c=preds, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = EM(n_components=8)\n",
    "km.fit(ica_wine)\n",
    "preds = km.predict(ica_wine)\n",
    "plt.scatter(ica_wine[:,0], ica_wine[:,1], c=preds, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = EM(n_components=8)\n",
    "km.fit(rca_wine)\n",
    "preds = km.predict(rca_wine)\n",
    "plt.scatter(rca_wine[:,0], rca_wine[:,1], c=preds, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = EM(n_components=8)\n",
    "km.fit(rfc_wine)\n",
    "preds = km.predict(rfc_wine)\n",
    "plt.scatter(rfc_wine[:,0], rfc_wine[:,1], c=preds, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_default, topcols_default = run_RFC(X_default, y_default, df_default_scaled)\n",
    "pca_default = PCA(n_components = 15,random_state=5).fit_transform(X_default)\n",
    "ica_default = ICA(n_components = 25,random_state=5).fit_transform(X_default)\n",
    "rca_default = ICA(n_components = 20,random_state=5).fit_transform(X_default)\n",
    "rfc_default = df_default_scaled[topcols_default]\n",
    "rfc_default = np.array(rfc_default.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss_default_pca = run_kmeans(pca_default,y_default,'PCA Default Data')\n",
    "ss_default_ica = run_kmeans(ica_default,y_default,'ICA Default Data')\n",
    "ss_default_rca = run_kmeans(rca_default,y_default,'RCA Default Data')\n",
    "ss_default_rfc = run_kmeans(rfc_default,y_default,'RFC Default Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squared_distance_comparison(default_squared_distances, ss_default_pca, 'pca')\n",
    "squared_distance_comparison(default_squared_distances, ss_default_ica, 'ica')\n",
    "squared_distance_comparison(default_squared_distances, ss_default_rca, 'rca')\n",
    "squared_distance_comparison(default_squared_distances, ss_default_rfc, 'rfc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aic_default_pca, bic_default_pca, log_like_default_pca = run_EM(pca_default,y_default,'PCA default Data')\n",
    "aic_default_ica, bic_default_ica, log_like_default_ica  = run_EM(ica_default,y_default,'ICA default Data')\n",
    "aic_default_rca, bic_default_rca, log_like_default_rca  = run_EM(rca_default,y_default,'RCA default Data')\n",
    "aic_default_rfc, bic_default_rfc, log_like_default_rfc  = run_EM(rfc_default,y_default,'RFC default Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "squared_distance_comparison(aic=[default_aic, aic_default_pca], bic=[default_bic, bic_default_pca], transformation='pca', method='em')\n",
    "squared_distance_comparison(aic=[default_aic, aic_default_ica], bic=[default_bic, bic_default_ica], transformation='ica', method='em')\n",
    "squared_distance_comparison(aic=[default_aic, aic_default_rca], bic=[default_bic, bic_default_rca], transformation='rca', method='em')\n",
    "squared_distance_comparison(aic=[default_aic, aic_default_rfc], bic=[default_bic, bic_default_rfc], transformation='rfc', method='em')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN on Projected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Original, full dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X_wine),np.array(y_wine), test_size=0.20)\n",
    "full_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_full, NN_train_score_full, NN_fit_time_full, NN_pred_time_full = plot_learning_curve(full_est, X_train, y_train,title=\"Neural Net Phishing: Full\")\n",
    "final_classifier_evaluation(full_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(pca_wine),np.array(y_wine), test_size=0.20)\n",
    "pca_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_pca, NN_train_score_pca, NN_fit_time_pca, NN_pred_time_pca = plot_learning_curve(pca_est, X_train, y_train,title=\"Neural Net Phishing: PCA\")\n",
    "final_classifier_evaluation(pca_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(ica_wine),np.array(y_wine), test_size=0.20)\n",
    "ica_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_ica, NN_train_score_ica, NN_fit_time_ica, NN_pred_time_ica = plot_learning_curve(ica_est, X_train, y_train,title=\"Neural Net Phishing: ICA\")\n",
    "final_classifier_evaluation(ica_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(rca_wine),np.array(y_wine), test_size=0.20)\n",
    "rca_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_rca, NN_train_score_rca, NN_fit_time_rca, NN_pred_time_rca = plot_learning_curve(rca_est, X_train, y_train,title=\"Neural Net Phishing: RCA\")\n",
    "final_classifier_evaluation(rca_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(rfc_wine),np.array(y_wine), test_size=0.20)\n",
    "rfc_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_rfc, NN_train_score_rfc, NN_fit_time_rfc, NN_pred_time_rfc = plot_learning_curve(rfc_est, X_train, y_train,title=\"Neural Net Phishing: RFC\")\n",
    "final_classifier_evaluation(rfc_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_fit_time(n,full_fit,pca_fit,ica_fit,rca_fit,rfc_fit,title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Model Training Times: \" + title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model Training Time (s)\")\n",
    "    plt.plot(n, full_fit, '-', color=\"k\", label=\"Full Dataset\")\n",
    "    plt.plot(n, pca_fit, '-', color=\"b\", label=\"PCA\")\n",
    "    plt.plot(n, ica_fit, '-', color=\"r\", label=\"ICA\")\n",
    "    plt.plot(n, rca_fit, '-', color=\"g\", label=\"RCA\")\n",
    "    plt.plot(n, rfc_fit, '-', color=\"m\", label=\"RFC\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "def compare_pred_time(n,full_pred, pca_pred, ica_pred, rca_pred, rfc_pred, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Model Prediction Times: \" + title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model Prediction Time (s)\")\n",
    "    plt.plot(n, full_pred, '-', color=\"k\", label=\"Full Dataset\")\n",
    "    plt.plot(n, pca_pred, '-', color=\"b\", label=\"PCA\")\n",
    "    plt.plot(n, ica_pred, '-', color=\"r\", label=\"ICA\")\n",
    "    plt.plot(n, rca_pred, '-', color=\"g\", label=\"RCA\")\n",
    "    plt.plot(n, rfc_pred, '-', color=\"m\", label=\"RFC\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_learn_time(n,full_learn, pca_learn, ica_learn, rca_learn, rfc_learn, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Model Learning Rates: \" + title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model F1 Score\")\n",
    "    plt.plot(n, full_learn, '-', color=\"k\", label=\"Full Dataset\")\n",
    "    plt.plot(n, pca_learn, '-', color=\"b\", label=\"PCA\")\n",
    "    plt.plot(n, ica_learn, '-', color=\"r\", label=\"ICA\")\n",
    "    plt.plot(n, rca_learn, '-', color=\"g\", label=\"RCA\")\n",
    "    plt.plot(n, rfc_learn, '-', color=\"m\", label=\"RFC\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_fit_time(train_samp_full, NN_fit_time_full, NN_fit_time_pca, NN_fit_time_ica, \n",
    "                 NN_fit_time_rca, NN_fit_time_rfc, 'Wine Dataset')              \n",
    "compare_pred_time(train_samp_full, NN_pred_time_full, NN_pred_time_pca, NN_pred_time_ica, \n",
    "                 NN_pred_time_rca, NN_pred_time_rfc, 'Wine Dataset')   \n",
    "compare_learn_time(train_samp_full, NN_train_score_full, NN_train_score_pca, NN_train_score_ica, \n",
    "                 NN_train_score_rca, NN_train_score_rfc, 'Wine Dataset')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projected Data with cluster labels on wine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addclusters(X,km_lables,em_lables):\n",
    "    \n",
    "    df = pd.DataFrame(X)\n",
    "    df['KM Cluster'] = km_labels\n",
    "    df['EM Cluster'] = em_labels\n",
    "    #col_1hot = ['KM Cluster', 'EM Cluster']\n",
    "    #df_1hot = df[col_1hot]\n",
    "    #df_1hot = pd.get_dummies(df_1hot).astype('category')\n",
    "    #df_others = df.drop(col_1hot,axis=1)\n",
    "    #df = pd.concat([df_others,df_1hot],axis=1)\n",
    "    new_X = np.array(df.values)   \n",
    "    \n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5,n_init=10,random_state=100,n_jobs=-1).fit(X_wine)\n",
    "km_labels = km.labels_\n",
    "em = EM(n_components=8,covariance_type='diag',n_init=1,warm_start=True,random_state=100).fit(X_wine)\n",
    "em_labels = em.predict(X_wine)\n",
    "\n",
    "clust_full = addclusters(X_wine,km_labels,em_labels)\n",
    "clust_pca = addclusters(pca_wine,km_labels,em_labels)\n",
    "clust_ica = addclusters(ica_wine,km_labels,em_labels)\n",
    "clust_rca = addclusters(rca_wine,km_labels,em_labels)\n",
    "clust_rfc = addclusters(rfc_wine,km_labels,em_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Original, full dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(clust_full),np.array(y_wine), test_size=0.20)\n",
    "full_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_full, NN_train_score_full, NN_fit_time_full, NN_pred_time_full = plot_learning_curve(full_est, X_train, y_train,title=\"Neural Net Phishing with Clusters: Full\")\n",
    "final_classifier_evaluation(full_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(clust_pca),np.array(y_wine), test_size=0.20)\n",
    "pca_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_pca, NN_train_score_pca, NN_fit_time_pca, NN_pred_time_pca = plot_learning_curve(pca_est, X_train, y_train,title=\"Neural Net Wine with Clusters: PCA\")\n",
    "final_classifier_evaluation(pca_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(clust_ica),np.array(y_wine), test_size=0.20)\n",
    "ica_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_ica, NN_train_score_ica, NN_fit_time_ica, NN_pred_time_ica = plot_learning_curve(ica_est, X_train, y_train,title=\"Neural Net Wine with Clusters: ICA\")\n",
    "final_classifier_evaluation(ica_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(clust_rca),np.array(y_wine), test_size=0.20)\n",
    "rca_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_rca, NN_train_score_rca, NN_fit_time_rca, NN_pred_time_rca = plot_learning_curve(rca_est, X_train, y_train,title=\"Neural Net Wine with Clusters: RCA\")\n",
    "final_classifier_evaluation(rca_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(clust_rfc),np.array(y_wine), test_size=0.20)\n",
    "rfc_est = MLPClassifier(hidden_layer_sizes=(50,), solver='adam', activation='logistic', learning_rate_init=0.01, random_state=100)\n",
    "train_samp_rfc, NN_train_score_rfc, NN_fit_time_rfc, NN_pred_time_rfc = plot_learning_curve(rfc_est, X_train, y_train,title=\"Neural Net Wine with Clusters: RFC\")\n",
    "final_classifier_evaluation(rfc_est, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_fit_time(train_samp_full, NN_fit_time_full, NN_fit_time_pca, NN_fit_time_ica, \n",
    "                 NN_fit_time_rca, NN_fit_time_rfc, 'Wine Dataset')              \n",
    "compare_pred_time(train_samp_full, NN_pred_time_full, NN_pred_time_pca, NN_pred_time_ica, \n",
    "                 NN_pred_time_rca, NN_pred_time_rfc, 'Wine Dataset')   \n",
    "compare_learn_time(train_samp_full, NN_train_score_full, NN_train_score_pca, NN_train_score_ica, \n",
    "                 NN_train_score_rca, NN_train_score_rfc, 'Wine Dataset')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
